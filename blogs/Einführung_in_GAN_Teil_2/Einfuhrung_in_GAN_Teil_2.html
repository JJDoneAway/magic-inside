<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLTi9jKYd1gJzj5O2gWsEpXq5DSxTUXlwXebxyXtT0rCwv1unUalIzurR1sLOY-mh2Q);.lst-kix_list_2-6>li:before{content:"   "}.lst-kix_list_2-7>li:before{content:" "}.lst-kix_list_2-7>li{counter-increment:lst-ctn-kix_list_2-7}.lst-kix_list_3-7>li{counter-increment:lst-ctn-kix_list_3-7}ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-4>li:before{content:"   "}.lst-kix_list_2-5>li:before{content:"   "}.lst-kix_list_2-8>li:before{content:" "}.lst-kix_list_3-0>li:before{content:"\002022   "}.lst-kix_list_3-1>li:before{content:"\002013   "}.lst-kix_list_3-2>li:before{content:"\002022   "}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 -1}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_3-1{list-style-type:none}.lst-kix_list_3-5>li:before{content:"\002013   "}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\002022   "}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_3-0{list-style-type:none}ol.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\002013   "}ul.lst-kix_list_3-5{list-style-type:none}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ol.lst-kix_list_3-8.start{counter-reset:lst-ctn-kix_list_3-8 -1}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ol.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}ol.lst-kix_list_3-8{list-style-type:none}.lst-kix_list_3-8>li:before{content:" "}.lst-kix_list_3-8>li{counter-increment:lst-ctn-kix_list_3-8}.lst-kix_list_3-6>li:before{content:"\002022   "}.lst-kix_list_3-7>li:before{content:" "}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 -1}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}ol.lst-kix_list_3-7.start{counter-reset:lst-ctn-kix_list_3-7 -1}ol.lst-kix_list_2-8.start{counter-reset:lst-ctn-kix_list_2-8 -1}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_1-0>li:before{content:"   "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"   "}.lst-kix_list_1-2>li:before{content:"   "}ol.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ol.lst-kix_list_2-8{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"   "}.lst-kix_list_1-4>li:before{content:"   "}.lst-kix_list_1-7>li:before{content:" "}ol.lst-kix_list_2-7.start{counter-reset:lst-ctn-kix_list_2-7 -1}.lst-kix_list_1-5>li:before{content:"   "}.lst-kix_list_1-6>li:before{content:"   "}li.li-bullet-0:before{margin-left:-24pt;white-space:nowrap;display:inline-block;min-width:24pt}.lst-kix_list_2-0>li:before{content:"   "}.lst-kix_list_2-1>li:before{content:"   "}.lst-kix_list_2-8>li{counter-increment:lst-ctn-kix_list_2-8}.lst-kix_list_1-8>li:before{content:" "}.lst-kix_list_2-2>li:before{content:"   "}.lst-kix_list_2-3>li:before{content:"   "}ol{margin:0;padding:0}table td,table th{padding:0}.c9{margin-left:18pt;padding-top:1.8pt;padding-left:6pt;padding-bottom:1.8pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c13{color:#60a0b0;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:italic}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:normal}.c4{color:#40a070;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:normal}.c20{color:#4f81bd;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c15{color:#4f81bd;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Calibri";font-style:normal}.c22{padding-top:24pt;padding-bottom:0pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:italic}.c17{color:#4f81bd;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Calibri";font-style:normal}.c2{color:#4070a0;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:normal}.c11{padding-top:10pt;padding-bottom:0pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{color:#19177c;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:normal}.c19{color:#335b8a;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Calibri";font-style:normal}.c7{padding-top:0pt;padding-bottom:10pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c16{color:#007020;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:normal}.c10{padding-top:0pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c6{padding-top:9pt;padding-bottom:9pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c8{padding-top:0pt;padding-bottom:10pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c21{background-color:#ffffff;max-width:470.3pt;padding:70.8pt 70.8pt 56.7pt 70.8pt}.c12{padding:0;margin:0}.c18{color:inherit;text-decoration:inherit}.title{padding-top:24pt;color:#335b8a;font-weight:700;font-size:18pt;padding-bottom:12pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:center}.subtitle{padding-top:12pt;color:#335b8a;font-weight:700;font-size:15pt;padding-bottom:12pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:center}li{color:#000000;font-size:12pt;font-family:"Cambria"}p{margin:0;color:#000000;font-size:12pt;font-family:"Cambria"}h1{padding-top:24pt;color:#335b8a;font-weight:700;font-size:16pt;padding-bottom:0pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:10pt;color:#4f81bd;font-weight:700;font-size:16pt;padding-bottom:0pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:10pt;color:#4f81bd;font-weight:700;font-size:14pt;padding-bottom:0pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:10pt;color:#4f81bd;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:10pt;color:#4f81bd;font-size:12pt;padding-bottom:0pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#4f81bd;font-size:12pt;padding-bottom:0pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c21 doc-content"><a id="id.gjdgxs"></a><h1 class="c22"><span class="c19">GAN - Teil 2 - Keras kann auch Ziffern schreiben</span></h1><a id="id.30j0zll"></a><h2 class="c11"><span class="c15">Worum geht es in diesem Teil</span></h2><p class="c6"><span class="c3">Nachdem ich im ersten Teil den grundlegenden Trick der GAN beschrieben habe, m&ouml;chte ich hier ein einfaches GAN bauen, dass in der Lage ist, die handschriftlichen Ziffern aus der MNIST Datenbank t&auml;uschend echt nachzumachen. Ich verwende f&uuml;r diesen &bdquo;Zaubertrank&ldquo; nur die elementaren Zutaten aus der Speisekammer der neuronalen Netze um mich auf die GAN Spezifika konzentrieren zu k&ouml;nnen. Nichts desto Trotz ist das Ergebnis schon recht beeindruckend. Wenn ich hier von Zaubertrank und Speisekammer rede, so geschieht das nicht ganz ohne Grund. In der Literatur wird gerne der Ausdruck &quot;Alchemie der GAN&quot; verwendet, der darauf hinweisen soll, dass man viel Erfahrung und Gesp&uuml;r beim Bau von GAN&#39;s ben&ouml;tigt. Das liegt daran, dass wir mit unseren neuronalen Netzen mittels Gradientenverfahren nicht einem festen Minima entgegenschreiten, sondern wir suchen ein Gleichgewicht zwischen G(z) und D(x). Dadurch verschieben sich die Minima in den Netzten nach jedem Iterationsschritt. Die Aufgabe beim Bau von GAN&#39;s ist es also seine Netzte so zu bauen, dass das Gleichgewicht leicht zu finden ist. Man braucht ein Netz wie eine tiefe Sch&uuml;ssel, in der eine kleine Kugel automatisch dem tiefsten Punkt und damit seinem Gleichgewicht zustrebt. Das andere Extrem ist ein Netz wie ein Bleistift, den man auf der Spitze ausbalancieren m&ouml;chte, was bekanntlich schnell mal frustrieren kann. H&auml;ngen wir uns also unsere Druidenm&auml;ntel um und greifen zu unseren goldenen Sicheln &#128522;</span></p><a id="id.1fob9te"></a><h2 class="c11"><span class="c15">Die l&auml;stigen Vorbereitungen</span></h2><p class="c6"><span class="c3">Wie immer sammle ich alle n&ouml;tigen Importstatements in einem Block</span></p><p class="c8"><span class="c13">#tensorflow</span><span class="c3"><br></span><span class="c0">import tensorflow as tf</span><span class="c3"><br></span><span class="c0">from tensorflow.python.client import device_lib</span><span class="c3"><br></span><span class="c0">from keras.backend.tensorflow_backend import set_session</span><span class="c3"><br><br></span><span class="c0">import keras</span><span class="c3"><br></span><span class="c0">from keras.layers import Input</span><span class="c3"><br></span><span class="c0">from keras.models import Model, Sequential</span><span class="c3"><br></span><span class="c0">from keras.layers.core import Dense, Dropout</span><span class="c3"><br></span><span class="c0">from keras.layers.advanced_activations import LeakyReLU</span><span class="c3"><br></span><span class="c0">from keras.datasets import mnist</span><span class="c3"><br></span><span class="c0">from keras.optimizers import Adam, RMSprop</span><span class="c3"><br></span><span class="c0">from keras import initializers</span><span class="c3"><br><br></span><span class="c0">config </span><span class="c5">=</span><span class="c0">&nbsp;tf.ConfigProto()</span><span class="c3"><br></span><span class="c0">config.gpu_options.allow_growth </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c1">True</span><span class="c0">&nbsp; </span><span class="c13"># dynamically grow the memory used on the GPU</span><span class="c3"><br></span><span class="c0">config.log_device_placement </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c1">True</span><span class="c0">&nbsp; </span><span class="c13"># to log device placement (on which device the operation ran)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># (nothing gets printed in Jupyter, only if you run it standalone)</span><span class="c3"><br></span><span class="c0">sess </span><span class="c5">=</span><span class="c0">&nbsp;tf.Session(config</span><span class="c5">=</span><span class="c0">config)</span><span class="c3"><br></span><span class="c0">set_session(sess) &nbsp;</span><span class="c13"># set this TensorFlow session as the default session for Keras</span><span class="c3"><br></span><span class="c0">print(</span><span class="c2">&quot;TensorFlow Version:\t{}&quot;</span><span class="c0">.format(tf.__version__))</span><span class="c3"><br></span><span class="c0">print(</span><span class="c2">&quot;Keras Version:\t\t{}&quot;</span><span class="c0">.format(keras.__version__))</span><span class="c3"><br></span><span class="c0">print(</span><span class="c2">&quot;Found GPU &amp; CPU devices:\n{}&quot;</span><span class="c0">.format(device_lib.list_local_devices()))</span><span class="c3"><br><br><br></span><span class="c0">import numpy as np</span><span class="c3"><br></span><span class="c0">import matplotlib.pyplot as plt</span><span class="c3"><br><br></span><span class="c13"># ein h&uuml;bsches Tool f&uuml;r Progerss Bars</span><span class="c3"><br></span><span class="c0">from tqdm import tqdm</span><span class="c3"><br><br></span><span class="c13"># F&uuml;r das Rauschen brauchen wir Zufallszahlen</span><span class="c3"><br></span><span class="c0">np.random.seed(</span><span class="c4">911972</span><span class="c0">)</span></p><a id="id.3znysh7"></a><h2 class="c11"><span class="c15">Daten vorbereiten</span></h2><ul class="c12 lst-kix_list_3-0 start"><li class="c9 li-bullet-0"><span class="c3">Zuerst besorge ich mir die Testdaten auf dem &uuml;blichen Weg.</span></li><li class="c9 li-bullet-0"><span class="c3">Da ich hier wirklich die einfachsten Netze verwenden m&ouml;chte (also simple Dense Netze), sammle ich alle Pixelinformationen nicht in einer zwei dimensionalen 28 x 28 Matrix, sondern in einem 28 x 28 = 784 Elemente gro&szlig;en Vektor.</span></li><li class="c9 li-bullet-0"><span class="c3">Ein erster Erfahrungswert ist es, den Tangens Hyperbolicus als finale Aktivierungsfunktion zu verwenden. Deshalb normiere ich meine Daten auf das Intervall von [-1, 1].</span></li><li class="c9 li-bullet-0"><span class="c3">Die Gr&ouml;&szlig;e des Input Vektors f&uuml;r den Generator setzte ich Zuf&auml;llig auf 64</span></li><li class="c9 li-bullet-0"><span class="c3">Die Optimierung der Hyperparameter ist die wesentliche Flei&szlig;arbeit. Somit sammle ich sie hier in einem Block zusammen.</span></li></ul><p class="c8"><span class="c0">(real_images_all, __), (__, __) </span><span class="c5">=</span><span class="c0">&nbsp;mnist.load_data()</span><span class="c3"><br></span><span class="c0">print(</span><span class="c2">&#39;1. Rohdaten:&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">print(real_images_all[</span><span class="c4">0</span><span class="c0">, </span><span class="c4">14</span><span class="c0">, </span><span class="c4">10</span><span class="c0">:</span><span class="c4">20</span><span class="c0">])</span><span class="c3"><br><br></span><span class="c0">real_images_all </span><span class="c5">=</span><span class="c0">&nbsp;real_images_all.reshape(</span><span class="c4">60000</span><span class="c0">, </span><span class="c4">784</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">print(</span><span class="c2">&#39;2. Zum Vektor aufgebogen:&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">print(real_images_all[</span><span class="c4">0</span><span class="c0">, </span><span class="c4">14</span><span class="c5">*</span><span class="c4">28</span><span class="c0">&nbsp;</span><span class="c5">+</span><span class="c4">10</span><span class="c0">&nbsp;: </span><span class="c4">14</span><span class="c5">*</span><span class="c4">28</span><span class="c0">&nbsp;</span><span class="c5">+</span><span class="c0">&nbsp;</span><span class="c4">20</span><span class="c0">])</span><span class="c3"><br><br></span><span class="c0">real_images_all </span><span class="c5">=</span><span class="c0">&nbsp;real_images_all.astype(</span><span class="c2">&#39;float32&#39;</span><span class="c0">) &nbsp;</span><span class="c5">/</span><span class="c0">&nbsp;</span><span class="c4">255</span><span class="c0">&nbsp;</span><span class="c5">*</span><span class="c0">&nbsp;</span><span class="c4">2</span><span class="c0">&nbsp;</span><span class="c5">-</span><span class="c0">&nbsp;</span><span class="c4">1</span><span class="c3"><br></span><span class="c0">print(</span><span class="c2">&#39;3. in das Interval [-1, 1] normiert:&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">print(real_images_all[</span><span class="c4">0</span><span class="c0">, </span><span class="c4">14</span><span class="c5">*</span><span class="c4">28</span><span class="c0">&nbsp;</span><span class="c5">+</span><span class="c4">10</span><span class="c0">&nbsp;: </span><span class="c4">14</span><span class="c5">*</span><span class="c4">28</span><span class="c0">&nbsp;</span><span class="c5">+</span><span class="c0">&nbsp;</span><span class="c4">20</span><span class="c0">])</span><span class="c3"><br><br></span><span class="c0">z_size </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">64</span><span class="c3"><br><br></span><span class="c13"># Da ich f&uuml;r alle Netzte einen Optimizer brauche, der ein wenig sanfter als der Default eingestellt ist, monfiguriere ich ihn einmal</span><span class="c3"><br></span><span class="c13">#optimizer= Adam(lr=0.0002, beta_1=0.5)</span><span class="c3"><br></span><span class="c0">optimizer</span><span class="c5">=</span><span class="c0">&nbsp;RMSprop(lr</span><span class="c5">=</span><span class="c4">0.0008</span><span class="c0">, clipvalue</span><span class="c5">=</span><span class="c4">1.0</span><span class="c0">, decay</span><span class="c5">=</span><span class="c4">1e-8</span><span class="c0">)</span><span class="c3"><br><br></span><span class="c13"># dropout ist wichtig bei GAN</span><span class="c3"><br></span><span class="c0">dropout_rate </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">0.2</span><span class="c3"><br><br></span><span class="c13"># die steigung der LeakyReLu im negativen Bereich</span><span class="c3"><br></span><span class="c0">leaky_faktor </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">0.2</span></p><a id="id.2et92p0"></a><h2 class="c11"><span class="c15">Der Generator</span></h2><p class="c6"><span class="c3">Im Prinzip ist der Generator das einfachste Netz, das man bauen kann. Eine kleine Besonderheit hat es aber dennoch. Ich verwende hier &sbquo;LeakyReLU&lsquo; als Ansatzfunktion. Seit einiger Zeit setzt sich ReLU als Allzweckwaffe unter den Ansatzfunktionen der Hidden - Layer durch. Allerdings hat sie eine kleine Schw&auml;che. Dadurch, dass sie den negativen Wertebereich auf Null abbildet, l&auml;sst sie Knoten schnell absterben. Ein einmal abgestorbener Knoten wird nicht mehr aktiviert, da auf ihn kein Fehler mehr aufgeteilt wird. Das wirkt sich negativ auf das &bdquo;tiefe&ldquo; propagieren von Fehlern aus. Mit der LeakyReLu wird dieses Problem repariert, indem man den negativen Wertebereich nicht ganz auf 0 abbildet.</span></p><p class="c6"><span class="c3">Als Aktivierungsfunktion des letzten Layers w&auml;hle ich wie bereits erw&auml;hnt den tanh als &bdquo;magische&ldquo; Zutat.</span></p><p class="c8"><span class="c0">generator </span><span class="c5">=</span><span class="c0">&nbsp;Sequential()</span><span class="c3"><br></span><span class="c0">generator.add(Dense(</span><span class="c4">256</span><span class="c0">, input_dim</span><span class="c5">=</span><span class="c0">z_size))</span><span class="c3"><br></span><span class="c0">generator.add(LeakyReLU(leaky_faktor))</span><span class="c3"><br><br></span><span class="c0">generator.add(Dense(</span><span class="c4">512</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">generator.add(LeakyReLU(leaky_faktor))</span><span class="c3"><br><br></span><span class="c0">generator.add(Dense(</span><span class="c4">1024</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">generator.add(LeakyReLU(leaky_faktor))</span><span class="c3"><br><br></span><span class="c0">generator.add(Dense(</span><span class="c4">784</span><span class="c0">, activation</span><span class="c5">=</span><span class="c2">&#39;tanh&#39;</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">generator.compile(loss</span><span class="c5">=</span><span class="c2">&#39;binary_crossentropy&#39;</span><span class="c0">, optimizer</span><span class="c5">=</span><span class="c0">optimizer)</span><span class="c3"><br></span><span class="c0">generator.summary()</span></p><a id="id.tyjcwt"></a><h2 class="c11"><span class="c15">Der Diskriminator</span></h2><p class="c6"><span class="c3">Der Diskriminator ist &auml;hnlich schlicht aufgebaut, wie der Generator. Die Input Dimension ist 28 x 28, also jeweils ein Bild. Die Output Dimension ist ein Skalar zwischen Null und Eins. Der Diskriminator verf&uuml;gt anders als der Generator &uuml;ber Dropout Layer, um auch hier den Zufall mit einzubinden. Der Zufall verhindert im GAN, dass sich die Netze zu schnell einschn&uuml;ren und in einen d&uuml;nn besiedelten Zustand gleiten. Das ist bei GAN&rsquo;s sehr schlecht, da die &bdquo;Kreativit&auml;t&ldquo; in der Vielfalt der M&ouml;glichkeiten liegt. Somit m&uuml;ssen m&ouml;glichst viele Gewichte aktiviert sein um eine m&ouml;glichst breite Varianz zu bekommen. Der Diskriminator bekommt seine Vielfalt &uuml;brigens nicht durch Dropout Layer, sondern durch das Rauschen des Input Vektors z.</span></p><p class="c8"><span class="c0">discriminator </span><span class="c5">=</span><span class="c0">&nbsp;Sequential()</span><span class="c3"><br></span><span class="c0">discriminator.add(Dense(</span><span class="c4">1024</span><span class="c0">, input_dim</span><span class="c5">=</span><span class="c4">784</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">discriminator.add(LeakyReLU(leaky_faktor))</span><span class="c3"><br></span><span class="c0">discriminator.add(Dropout(dropout_rate))</span><span class="c3"><br><br></span><span class="c0">discriminator.add(Dense(</span><span class="c4">512</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">discriminator.add(LeakyReLU(leaky_faktor))</span><span class="c3"><br></span><span class="c0">discriminator.add(Dropout(dropout_rate))</span><span class="c3"><br><br></span><span class="c0">discriminator.add(Dense(</span><span class="c4">256</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">discriminator.add(LeakyReLU(leaky_faktor))</span><span class="c3"><br></span><span class="c0">discriminator.add(Dropout(dropout_rate))</span><span class="c3"><br><br></span><span class="c0">discriminator.add(Dense(</span><span class="c4">1</span><span class="c0">, activation</span><span class="c5">=</span><span class="c2">&#39;sigmoid&#39;</span><span class="c0">))</span><span class="c3"><br></span><span class="c0">discriminator.compile(loss</span><span class="c5">=</span><span class="c2">&#39;binary_crossentropy&#39;</span><span class="c0">, optimizer</span><span class="c5">=</span><span class="c0">optimizer)</span><span class="c3"><br></span><span class="c0">discriminator.summary()</span></p><a id="id.3dy6vkm"></a><h2 class="c11"><span class="c15">Das GAN</span></h2><p class="c6"><span class="c3">Um den Generator trainieren zu k&ouml;nnen, muss man das GAN zusammensetzen. Dazu nutzt man bei Keras das funktionale API. Damit kann man einem Modell weitere Layer hinzuf&uuml;gen. So wird hier der Input Layer auf den Generator und dieser auf den Diskriminator geschichtet. Danach ruft man den Modell Konstruktor auf, um das GAN zu bekommen. &Uuml;brigens spiegelt sich in der funktionalen Schreibweise von Python die theoretische Herleitung der GAN&rsquo;s wieder D(G(z)).</span></p><p class="c8"><span class="c13"># der Diskriminator wird hier gleich eingefroren</span><span class="c3"><br></span><span class="c0">discriminator.trainable </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c1">False</span><span class="c3"><br><br></span><span class="c13"># der verrauschte Input Vektor</span><span class="c3"><br></span><span class="c0">z </span><span class="c5">=</span><span class="c0">&nbsp;Input(shape</span><span class="c5">=</span><span class="c0">(z_size,))</span><span class="c3"><br><br></span><span class="c13"># Das GAN D(G(z))</span><span class="c3"><br></span><span class="c0">gan_core </span><span class="c5">=</span><span class="c0">&nbsp;discriminator(generator(z))</span><span class="c3"><br><br></span><span class="c13"># Um ein eigenst&auml;ndiges Netz zu erhalten, baut man sich ein neues Modell</span><span class="c3"><br></span><span class="c0">gan </span><span class="c5">=</span><span class="c0">&nbsp;Model(inputs </span><span class="c5">=</span><span class="c0">&nbsp;z, outputs </span><span class="c5">=</span><span class="c0">&nbsp;gan_core)</span><span class="c3"><br></span><span class="c0">gan.compile(loss </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c2">&#39;binary_crossentropy&#39;</span><span class="c0">, optimizer </span><span class="c5">=</span><span class="c0">&nbsp;optimizer)</span><span class="c3"><br></span><span class="c0">gan.summary()</span></p><a id="id.1t3h5sf"></a><h2 class="c11"><span class="c15">Das Training</span></h2><p class="c6"><span class="c3">Wie im Teil eins beschrieben werden hier folgende Schritte immer wieder abgearbeitet: 1. Zuerst generiert man mit dem Generator einen Stapel Fake &ndash; Bilder, wobei wir ein nach Gau&szlig; normalverteiltes Rauschen erzeugen. (Es ver&auml;ndert &uuml;brigens die Qualit&auml;t der Ergebnisse, wenn man die Werte &sigma; und &micro; feinabstimmt.) 2. Diese Bilder nimmt man zusammen mit gleich vielen echten Bildern, um den Diskriminator zu trainieren. Dadurch wird der Diskriminator ein wenig besser. 3. Das GAN trainiert man mit einem Stapel Rauschens und fordert, dass das Ergebnis immer </span><span class="c14">real</span><span class="c3">&nbsp;sein sollen, da wir ja die Realit&auml;t f&auml;lschen wollen. D(G(z)) = 1.</span></p><p class="c8"><span class="c0">epochs </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">5000</span><span class="c3"><br></span><span class="c0">batch_size </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">128</span><span class="c3"><br></span><span class="c0">batch_count </span><span class="c5">=</span><span class="c0">&nbsp;real_images_all.shape[</span><span class="c4">0</span><span class="c0">] </span><span class="c5">/</span><span class="c0">&nbsp;batch_size</span><span class="c3"><br><br></span><span class="c0">&sigma; </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">1</span><span class="c3"><br></span><span class="c0">&micro; </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">0</span><span class="c3"><br><br></span><span class="c0">history_g </span><span class="c5">=</span><span class="c0">&nbsp;[]</span><span class="c3"><br></span><span class="c0">history_d </span><span class="c5">=</span><span class="c0">&nbsp;[]</span><span class="c3"><br><br></span><span class="c16">for</span><span class="c0">&nbsp;e </span><span class="c16">in</span><span class="c0">&nbsp;range(</span><span class="c4">1</span><span class="c0">, epochs</span><span class="c5">+</span><span class="c4">1</span><span class="c0">):</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; print(</span><span class="c2">&#39;-&#39;</span><span class="c5">*</span><span class="c4">15</span><span class="c0">, </span><span class="c2">&#39;Epoch %d&#39;</span><span class="c0">&nbsp;</span><span class="c5">%</span><span class="c0">&nbsp;e, </span><span class="c2">&#39;-&#39;</span><span class="c5">*</span><span class="c4">15</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; </span><span class="c16">for</span><span class="c0">&nbsp;count </span><span class="c16">in</span><span class="c0">&nbsp;tqdm(range(int(batch_count))):</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 1.1 wir bauen &#39;bach_size&#39; viele z vektoren mit normalverteiltem Rauschen der L&auml;nge &#39;z_size&#39;</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; z </span><span class="c5">=</span><span class="c0">&nbsp;np.random.normal(&micro;, &sigma;, size</span><span class="c5">=</span><span class="c0">[batch_size, z_size])</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 1.2 daraus generieren wir Fake Bilder</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; fake_images </span><span class="c5">=</span><span class="c0">&nbsp;generator.predict(z)</span><span class="c3"><br><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2.1 dazu entnehmen wir eine gleich gro&szlig;e Anzahl an echten Bildern</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; real_images </span><span class="c5">=</span><span class="c0">&nbsp;real_images_all[np.random.randint(</span><span class="c4">0</span><span class="c0">, real_images_all.shape[</span><span class="c4">0</span><span class="c0">], size</span><span class="c5">=</span><span class="c0">batch_size)]</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2.2 beide Stapel zusammen ergeben die Trainingsmenge f&uuml;r den Diskriminator</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; x_dis </span><span class="c5">=</span><span class="c0">&nbsp;np.concatenate([real_images, fake_images])</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2.3 der Ergebnisvektor wird mit null also fake vorbelegt</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; y_dis </span><span class="c5">=</span><span class="c0">&nbsp;np.zeros(</span><span class="c4">2</span><span class="c5">*</span><span class="c0">batch_size)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2.4 die richtigen Ergebnisse werden nicht mit 1 sondern mit 0.9 belegt. </span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># &nbsp; &nbsp; Das nennt sich &quot;one-sided label smoothing&quot; und dient dazu die &quot;Fr&uuml;chte&quot; </span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># &nbsp; &nbsp; nicht allzu hoch zu h&auml;ngen </span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; y_dis[batch_size:] </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">0.9</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2.5 den Diskriminator auffrieren</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; discriminator.trainable </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c1">True</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2.6 und trainieren</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; history_d.append(discriminator.train_on_batch(x_dis, y_dis))</span><span class="c3"><br><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 3.1 Wieder eine Stapel Rauschen bauen</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; x_gan </span><span class="c5">=</span><span class="c0">&nbsp;np.random.normal(&micro;, &sigma;, size</span><span class="c5">=</span><span class="c0">[batch_size, z_size])</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 3.2 Um echte Bilder vorzut&auml;uschen belegen wir die Antwort mit Null</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; y_gan </span><span class="c5">=</span><span class="c0">&nbsp;np.zeros(batch_size)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 3.3 Den diskriminator einfrieren, um seine Gewichte nicht zu &auml;ndern</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; discriminator.trainable </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c1">False</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 3.4 und so den generator mittelbar als Teil des GANs trainieren</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; history_g.append(gan.train_on_batch(x_gan, y_gan))</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; </span><span class="c13"># alle 20 Schritte werfe ich eine kleine Auswertung aus &nbsp; &nbsp;</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; </span><span class="c16">if</span><span class="c0">&nbsp;e </span><span class="c5">==</span><span class="c0">&nbsp;</span><span class="c4">1</span><span class="c0">&nbsp;</span><span class="c16">or</span><span class="c0">&nbsp;e </span><span class="c5">%</span><span class="c0">&nbsp;</span><span class="c4">20</span><span class="c0">&nbsp;</span><span class="c5">==</span><span class="c0">&nbsp;</span><span class="c4">0</span><span class="c0">:</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 1. Die entwicklung der Verlusraten beider Trainings</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.plot(history_g, </span><span class="c2">&#39;r&#39;</span><span class="c0">, label</span><span class="c5">=</span><span class="c2">&#39;generator loss&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.plot(history_d, </span><span class="c2">&#39;b&#39;</span><span class="c0">, label</span><span class="c5">=</span><span class="c2">&#39;discriminator loss&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.title(</span><span class="c2">&#39;Training of D(x) and D((g(z)))&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.legend()</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.show()</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.close()</span><span class="c3"><br><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; examples </span><span class="c5">=</span><span class="c0">&nbsp;</span><span class="c4">100</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; dim</span><span class="c5">=</span><span class="c0">(</span><span class="c4">10</span><span class="c0">, </span><span class="c4">10</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; figsize</span><span class="c5">=</span><span class="c0">(</span><span class="c4">10</span><span class="c0">, </span><span class="c4">10</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; noise </span><span class="c5">=</span><span class="c0">&nbsp;np.random.normal(&micro;, &sigma;, size</span><span class="c5">=</span><span class="c0">[examples, z_size])</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; generated_images </span><span class="c5">=</span><span class="c0">&nbsp;generator.predict(noise)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; generated_images </span><span class="c5">=</span><span class="c0">&nbsp;generated_images.reshape(examples, </span><span class="c4">28</span><span class="c0">, </span><span class="c4">28</span><span class="c0">)</span><span class="c3"><br><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># 2. Ein paar Demobilder</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.figure(figsize</span><span class="c5">=</span><span class="c0">figsize)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c16">for</span><span class="c0">&nbsp;i </span><span class="c16">in</span><span class="c0">&nbsp;range(generated_images.shape[</span><span class="c4">0</span><span class="c0">]):</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; plt.subplot(dim[</span><span class="c4">0</span><span class="c0">], dim[</span><span class="c4">1</span><span class="c0">], i</span><span class="c5">+</span><span class="c4">1</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; plt.imshow(generated_images[i], interpolation</span><span class="c5">=</span><span class="c2">&#39;nearest&#39;</span><span class="c0">, cmap</span><span class="c5">=</span><span class="c2">&#39;gray_r&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; plt.axis(</span><span class="c2">&#39;off&#39;</span><span class="c0">)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.tight_layout()</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.savefig(</span><span class="c2">&#39;output/gan_generated_image_epoch_%d.png&#39;</span><span class="c0">&nbsp;</span><span class="c5">%</span><span class="c0">&nbsp;e)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; plt.close()</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; </span><span class="c13"># alle 40 Schritte speicher ich das Modell des generators</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; </span><span class="c16">if</span><span class="c0">&nbsp;e </span><span class="c5">%</span><span class="c0">&nbsp;</span><span class="c4">40</span><span class="c0">&nbsp;</span><span class="c5">==</span><span class="c0">&nbsp;</span><span class="c4">0</span><span class="c0">:</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># Gewichte speichern</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; generator.save(</span><span class="c2">&#39;output_model/generator_model_%d.h5&#39;</span><span class="c0">&nbsp;</span><span class="c5">%</span><span class="c0">&nbsp;e)</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13"># Konfiguration speichern</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; model_json </span><span class="c5">=</span><span class="c0">&nbsp;generator.to_json()</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c16">with</span><span class="c0">&nbsp;open(</span><span class="c2">&quot;output_model/generator_model_{}.json&quot;</span><span class="c0">.format(e), </span><span class="c2">&quot;w&quot;</span><span class="c0">) as json_file:</span><span class="c3"><br></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; json_file.write(model_json)</span></p><a id="id.4d34og8"></a><h2 class="c11"><span class="c15">Schluss und wie geht es von hier weiter</span></h2><a id="id.2s8eyo1"></a><h3 class="c11"><span class="c17">Ergebnisse</span></h3><p class="c6"><span class="c3">Nach der ersten Epoche sehen die Ziffern noch so aus:</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 560.00px; height: 560.00px;"><img alt="Abb. 1: Ziffern nach der ersten Epoche" src="images/image1.png" style="width: 560.00px; height: 560.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c14">Abb. 1: Ziffern nach der ersten Epoche</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 516.51px; height: 356.96px;"><img alt="Abb. 2: Fehlerrate nach der ersten Epoche" src="images/image3.png" style="width: 516.51px; height: 356.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c14">Abb. 2: Fehlerrate nach der ersten Epoche</span></p><p class="c6"><span class="c3">Wenn man dem Netz nun ein wenig Zeit gibt, wird man schon nach wenigen Iterationen feststellen, dass Die Ergebnisse durchaus beeindruckend sind.</span></p><p class="c6"><span class="c3">Nach 300 Epochen sieht das Ergebnis dann so aus</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 560.00px; height: 560.00px;"><img alt="Abb. 3: Ziffern nach 300 Epochen" src="images/image2.png" style="width: 560.00px; height: 560.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c14">Abb. 3: Ziffern nach 300 Epochen</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.04px; height: 356.96px;"><img alt="Abb. 4: Fehlerrate nach 300 Epochen" src="images/image5.png" style="width: 507.04px; height: 356.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c14">Abb. 4: Fehlerrate nach 300 Epochen</span></p><p class="c6"><span class="c3">L&auml;sst man das Netz weiter trainieren, stellt man fest, dass sich das Ergebnis nicht mehr wesentlich verbessert. Hier das Ergebnis nach 700 Epochen:</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 560.00px; height: 560.00px;"><img alt="Abb. 5: Ziffern nach 700 Epochen" src="images/image4.png" style="width: 560.00px; height: 560.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c14">Abb. 5: Ziffern nach 700 Epochen</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.04px; height: 356.96px;"><img alt="Abb. 6: Fehlerrate nach 700 Epochen" src="images/image7.png" style="width: 507.04px; height: 356.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c14">Abb. 6: Fehlerrate nach 700 Epochen</span></p><p class="c6"><span class="c3">Anscheinend ist das Netz an seiner Leistungsgrenze und konvergiert wirklich in ein stabiles Gleichgewicht. Nat&uuml;rlich f&auml;llt auf, dass viele der Ziffern sehr geschmiert erscheinen. Dazu darf man aber nicht vergessen, dass auch die originalen Ziffern, die der Diskriminator zum Trainieren bekommen hat nicht gerade einen Sch&ouml;nschreibwettbewerb gewinnen w&uuml;rden: </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 560.00px; height: 568.56px;"><img alt="Abb. 7: Beispiel f&uuml;r die originalen Ziffern" src="images/image6.png" style="width: 560.00px; height: 568.56px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c3">Im Endeffekt erfindet das Netz nicht wirklich neue Ziffern, sondern bildet ein statistisches Mittel aus allen Bildern, mit denen es trainiert wurde und hebt Variationen aus dem Latenzraum, also den Raum der m&ouml;glichen richtigen Ziffern. Was man nun als folgenden Schritt noch machen sollte, sind zwei Pr&uuml;fungen:</span></p><a id="id.17dp8vu"></a><h3 class="c11"><span class="c17">Verteilung der Ziffern</span></h3><p class="c6"><span class="c3">Sind die erfundenen Ziffern gleich verteilt? Wenn ich also zum Beispiel 100.000 Ziffern erfinden lasse, so sollten etwa 10.000 St&uuml;ck pro einzelne Ziffer vorhanden sein.</span></p><a id="id.3rdcrjn"></a><h3 class="c11"><span class="c17">Trennsch&auml;rfe der Ziffern</span></h3><p class="c6"><span class="c3">Wie deutlich lassen sich die erfundenen Ziffern erkennen? In einem meiner letzten Blogs (</span><span class="c20"><a class="c18" href="https://www.google.com/url?q=https://www.mt-ag.com/so-entwirft-man-ein-top-cnn/&amp;sa=D&amp;source=editors&amp;ust=1737453521117737&amp;usg=AOvVaw3peaCYqYN2oh2PIooukbV9">So entwirft man ein Top CNN</a></span><span class="c3">) habe ich ein recht passables Netz implementiert, das Ziffern mit hoher Genauigkeit erkennt. Wenn die erfundenen Ziffern nun durch dieses Netz geschickt werden, kann man pr&uuml;fen, wie genau eine Ziffer zwischen Null und Neun erkannt wird. Ein m&ouml;gliches Ma&szlig; f&uuml;r diese Genauigkeit ist die von mir definierte Trennsch&auml;rfe </span><span class="c20"><a class="c18" href="https://www.google.com/url?q=https://www.mt-ag.com/trennschaerfe-bei-der-klassifizierung/&amp;sa=D&amp;source=editors&amp;ust=1737453521117958&amp;usg=AOvVaw2jgn4ifbMhAYwpb22KMQC6">Trennsch&auml;rfe bei der Klassifizierung</a></span></p><a id="id.26in1rg"></a><h3 class="c11"><span class="c17">Und weiter</span></h3><p class="c6"><span class="c3">Die Architektur um die GAN&rsquo;s hat sich schnell und tief entwickelt. Allerdings liegt der Fokus stark auf Bildern. Ich denke, man sollte auch Texten, Musik und Daten einer genaueren Untersuchung unterziehen, um auch hier Artefakte t&auml;uschend echt nachzuahmen. Alles in allem geht mit den GAN&rsquo;s f&uuml;r mich die Reise in Richtung Turing Test&hellip; </span></p><p class="c6" id="h.lnxbz9"><span class="c3">Ich w&uuml;nsche Dir viel Spa&szlig; beim Ausprobieren.</span></p></body></html>